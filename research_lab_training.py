# research_lab_training.py - ì—°êµ¬ì‹¤ ê³ ì‚¬ì–‘ ì»´í“¨í„°ìš© í•™ìŠµ ë° ê³ ê¸‰ ë¶„ì„ ì‹œìŠ¤í…œ
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
import cv2
import numpy as np
import json
import pandas as pd
import sqlite3
import requests
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import wandb  # Weights & Biases for experiment tracking
from ultralytics import YOLO
import gymnasium as gym
from stable_baselines3 import PPO, SAC
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback
import optuna  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
import warnings
warnings.filterwarnings('ignore')

class AquaponicsDataset(Dataset):
    """ì•„ì¿ ì•„í¬ë‹‰ìŠ¤ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, data_path, transform=None):
        self.data = pd.read_csv(data_path) if data_path.endswith('.csv') else pd.read_excel(data_path)
        self.transform = transform
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        
        # íŠ¹ì§• ì¶”ì¶œ (ê¸¸ì´, ë¬´ê²Œ, ì‹œê°„ ë“±)
        features = torch.tensor([
            row.get('length_cm', 0),
            row.get('weight_g', 0),
            row.get('height_cm', 0),
            row.get('confidence', 0),
            row.get('growth_rate', 0),
            row.get('variability', 0)
        ], dtype=torch.float32)
        
        # ë¼ë²¨ (ê±´ê°•ë„ ë˜ëŠ” ì„±ì¥ ì˜ˆì¸¡)
        label = torch.tensor(row.get('health_score', 1.0), dtype=torch.float32)
        
        return features, label

class GrowthPredictor(nn.Module):
    """ì„±ì¥ ì˜ˆì¸¡ ë”¥ëŸ¬ë‹ ëª¨ë¸"""
    
    def __init__(self, input_dim=6, hidden_dim=128, output_dim=1):
        super(GrowthPredictor, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, output_dim),
            nn.Sigmoid()  # 0-1 ë²”ìœ„ ì¶œë ¥
        )
        
    def forward(self, x):
        return self.network(x)

class AquaponicsEnvironment(gym.Env):
    """ê°•í™”í•™ìŠµì„ ìœ„í•œ ì•„ì¿ ì•„í¬ë‹‰ìŠ¤ í™˜ê²½"""
    
    def __init__(self):
        super(AquaponicsEnvironment, self).__init__()
        
        # ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤: [ì‚¬ë£ŒëŸ‰, ì¡°ëª…ê°•ë„, ìˆ˜ì˜¨ì¡°ì ˆ, pHì¡°ì ˆ]
        self.action_space = gym.spaces.Box(
            low=np.array([0.5, 0.3, 18.0, 6.0]),
            high=np.array([2.0, 1.0, 28.0, 8.0]),
            dtype=np.float32
        )
        
        # ìƒíƒœ ìŠ¤í˜ì´ìŠ¤: [ë¬¼ê³ ê¸°ìˆ˜, í‰ê· í¬ê¸°, ì‹ë¬¼ìˆ˜, í‰ê· ë†’ì´, í™˜ê²½íŒŒë¼ë¯¸í„°ë“¤]
        self.observation_space = gym.spaces.Box(
            low=np.array([0, 0, 0, 0, 18.0, 6.0, 0, 0]),
            high=np.array([50, 30, 20, 50, 28.0, 8.0, 1.0, 1.0]),
            dtype=np.float32
        )
        
        self.reset()
        
    def reset(self, seed=None, options=None):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        super().reset(seed=seed)
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        self.state = np.array([
            np.random.randint(5, 15),  # ë¬¼ê³ ê¸° ìˆ˜
            np.random.uniform(8, 12),  # í‰ê·  í¬ê¸°
            np.random.randint(3, 8),   # ì‹ë¬¼ ìˆ˜
            np.random.uniform(15, 25), # í‰ê·  ë†’ì´
            np.random.uniform(22, 26), # ìˆ˜ì˜¨
            np.random.uniform(6.5, 7.5), # pH
            np.random.uniform(0.7, 1.0), # ë¬¼ê³ ê¸° ê±´ê°•ë„
            np.random.uniform(0.7, 1.0)  # ì‹ë¬¼ ê±´ê°•ë„
        ], dtype=np.float32)
        
        self.step_count = 0
        self.max_steps = 100
        
        return self.state, {}
        
    def step(self, action):
        """í™˜ê²½ ìŠ¤í…"""
        self.step_count += 1
        
        # ì•¡ì…˜ ì ìš© (ì‚¬ë£ŒëŸ‰, ì¡°ëª…, ìˆ˜ì˜¨, pH)
        feed_amount, light_intensity, water_temp, ph_level = action
        
        # í™˜ê²½ ì—…ë°ì´íŠ¸
        self.state[4] = water_temp  # ìˆ˜ì˜¨
        self.state[5] = ph_level    # pH
        
        # ì„±ì¥ ì‹œë®¬ë ˆì´ì…˜
        growth_factor = self._calculate_growth_factor(action)
        
        # ë¬¼ê³ ê¸° ì„±ì¥
        self.state[1] += growth_factor * 0.1  # í¬ê¸° ì¦ê°€
        if np.random.random() < 0.05:  # ìƒˆ ë¬¼ê³ ê¸° í™•ë¥ 
            self.state[0] = min(50, self.state[0] + 1)
            
        # ì‹ë¬¼ ì„±ì¥
        self.state[3] += growth_factor * 0.2  # ë†’ì´ ì¦ê°€
        if np.random.random() < 0.03:  # ìƒˆ ì‹ë¬¼ í™•ë¥ 
            self.state[2] = min(20, self.state[2] + 1)
            
        # ê±´ê°•ë„ ì—…ë°ì´íŠ¸
        self.state[6] = np.clip(self.state[6] + growth_factor * 0.05 - 0.01, 0, 1)
        self.state[7] = np.clip(self.state[7] + growth_factor * 0.05 - 0.01, 0, 1)
        
        # ë³´ìƒ ê³„ì‚°
        reward = self._calculate_reward(action, growth_factor)
        
        # ì¢…ë£Œ ì¡°ê±´
        terminated = (self.step_count >= self.max_steps or 
                     self.state[6] < 0.3 or self.state[7] < 0.3)
        
        return self.state, reward, terminated, False, {}
        
    def _calculate_growth_factor(self, action):
        """ì„±ì¥ ì¸ì ê³„ì‚°"""
        feed_amount, light_intensity, water_temp, ph_level = action
        
        # ìµœì  ì¡°ê±´ê³¼ì˜ ì°¨ì´ ê³„ì‚°
        optimal_temp = 24.0
        optimal_ph = 7.0
        optimal_feed = 1.0
        optimal_light = 0.8
        
        temp_factor = 1.0 - abs(water_temp - optimal_temp) / 10.0
        ph_factor = 1.0 - abs(ph_level - optimal_ph) / 2.0
        feed_factor = 1.0 - abs(feed_amount - optimal_feed) / 1.0
        light_factor = 1.0 - abs(light_intensity - optimal_light) / 0.5
        
        return np.clip(np.mean([temp_factor, ph_factor, feed_factor, light_factor]), 0, 1)
        
    def _calculate_reward(self, action, growth_factor):
        """ë³´ìƒ í•¨ìˆ˜"""
        # ì„±ì¥ë¥  ë³´ìƒ
        growth_reward = growth_factor * 10
        
        # ì•ˆì •ì„± ë³´ìƒ (ê¸‰ê²©í•œ ë³€í™” í˜ë„í‹°)
        stability_penalty = np.sum(np.abs(action - [1.0, 0.8, 24.0, 7.0])) * 0.5
        
        # ê±´ê°•ë„ ë³´ìƒ
        health_reward = (self.state[6] + self.state[7]) * 5
        
        # ê°œì²´ìˆ˜ ë³´ìƒ
        population_reward = (self.state[0] + self.state[2]) * 0.1
        
        total_reward = growth_reward + health_reward + population_reward - stability_penalty
        
        return total_reward

class ResearchLabSystem:
    """ì—°êµ¬ì‹¤ ê³ ê¸‰ ë¶„ì„ ë° í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, server_url="http://192.168.1.200:5000"):
        self.server_url = server_url
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        self.db_path = 'research_data.db'
        self.init_research_db()
        
        # ëª¨ë¸ë“¤
        self.growth_predictor = None
        self.rl_agent = None
        self.yolo_model = None
        
        # ì‹¤í—˜ ì¶”ì 
        self.experiment_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def init_research_db(self):
        """ì—°êµ¬ìš© ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS experiments (
                id INTEGER PRIMARY KEY,
                experiment_id TEXT,
                model_type TEXT,
                hyperparameters TEXT,
                metrics TEXT,
                start_time DATETIME,
                end_time DATETIME,
                best_score REAL
            )
        ''')
        
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS predictions (
                id INTEGER PRIMARY KEY,
                experiment_id TEXT,
                object_id TEXT,
                prediction_type TEXT,
                predicted_value REAL,
                actual_value REAL,
                timestamp DATETIME,
                confidence REAL
            )
        ''')
        
        self.conn.commit()
        
    def fetch_training_data(self):
        """ì„œë²„ì—ì„œ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ìµœê·¼ í•œ ë‹¬ ë°ì´í„° ìš”ì²­
            response = requests.get(f"{self.server_url}/api/training_data")
            
            if response.status_code == 200:
                data = response.json()
                df = pd.DataFrame(data['records'])
                
                print(f"í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘: {len(df)}ê°œ ë ˆì½”ë“œ")
                return df
            else:
                print(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None
            
    def train_growth_predictor(self, data_df):
        """ì„±ì¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
        print("ì„±ì¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        features = ['length_cm', 'weight_g', 'height_cm', 'confidence', 'growth_rate', 'variability']
        target = 'future_growth'  # ë¯¸ë˜ ì„±ì¥ë¥ 
        
        # ë¯¸ë˜ ì„±ì¥ë¥  ê³„ì‚° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹œê³„ì—´ ë°ì´í„° í™œìš©)
        data_df['future_growth'] = data_df['growth_rate'].shift(-1).fillna(0)
        
        X = data_df[features].fillna(0)
        y = data_df[target].fillna(0)
        
        # ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_train), torch.FloatTensor(y_train.values)
        )
        val_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_val), torch.FloatTensor(y_val.values)
        )
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.growth_predictor = GrowthPredictor(input_dim=len(features))
        self.growth_predictor.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤í•¨ìˆ˜
        optimizer = optim.Adam(self.growth_predictor.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        # í•™ìŠµ ë£¨í”„
        best_val_loss = float('inf')
        patience = 10
        patience_counter = 0
        
        for epoch in range(100):
            # í›ˆë ¨
            self.growth_predictor.train()
            train_loss = 0
            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.growth_predictor(batch_x).squeeze()
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                
            # ê²€ì¦
            self.growth_predictor.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                    outputs = self.growth_predictor(batch_x).squeeze()
                    val_loss += criterion(outputs, batch_y).item()
                    
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            
            print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
            
            # ì¡°ê¸° ì¢…ë£Œ
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ëª¨ë¸ ì €ì¥
                torch.save(self.growth_predictor.state_dict(), f'growth_predictor_{self.experiment_id}.pth')
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print("ì¡°ê¸° ì¢…ë£Œ")
                    break
                    
        print(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
        
        # ì‹¤í—˜ ê¸°ë¡ ì €ì¥
        self.conn.execute('''
            INSERT INTO experiments (experiment_id, model_type, metrics, start_time, end_time, best_score)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            self.experiment_id,
            'GrowthPredictor',
            json.dumps({'val_loss': best_val_loss}),
            datetime.now() - timedelta(minutes=10),  # ëŒ€ëµì ì¸ ì‹œì‘ ì‹œê°„
            datetime.now(),
            best_val_loss
        ))
        self.conn.commit()
        
    def train_rl_agent(self):
        """ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í•™ìŠµ"""
        print("ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í•™ìŠµ ì‹œì‘")
        
        # í™˜ê²½ ìƒì„±
        env = make_vec_env(AquaponicsEnvironment, n_envs=4)
        
        # PPO ì—ì´ì „íŠ¸ ìƒì„±
        self.rl_agent = PPO(
            "MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            verbose=1,
            device=self.device
        )
        
        # í‰ê°€ ì½œë°±
        eval_env = AquaponicsEnvironment()
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=f'./rl_models/{self.experiment_id}/',
            log_path=f'./rl_logs/{self.experiment_id}/',
            eval_freq=10000,
            deterministic=True,
            render=False
        )
        
        # í•™ìŠµ ì‹¤í–‰
        self.rl_agent.learn(
            total_timesteps=100000,
            callback=eval_callback
        )
        
        # ëª¨ë¸ ì €ì¥
        self.rl_agent.save(f"rl_agent_{self.experiment_id}")
        
        print("ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í•™ìŠµ ì™„ë£Œ")
        
    def hyperparameter_optimization(self, data_df):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
        
        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
            lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)
            hidden_dim = trial.suggest_int('hidden_dim', 64, 512)
            dropout = trial.suggest_float('dropout', 0.1, 0.5)
            
            # ê°„ë‹¨í•œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (ì‹¤ì œë¡œëŠ” ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸)
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ëŒ€ì²´
            model = GrowthPredictor(hidden_dim=hidden_dim)
            
            # ê°€ìƒì˜ ì„±ëŠ¥ ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ê²€ì¦ ì†ì‹¤)
            score = np.random.random() * (1 - dropout) * (hidden_dim / 512) * (0.001 / lr)
            
            return score
            
        # Optuna ìŠ¤í„°ë””
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=50)
        
        print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {study.best_params}")
        print(f"ìµœê³  ì„±ëŠ¥: {study.best_value}")
        
        return study.best_params
        
    def anomaly_detection_analysis(self, data_df):
        """ì´ìƒ íƒì§€ ë¶„ì„"""
        print("ì´ìƒ íƒì§€ ë¶„ì„ ìˆ˜í–‰")
        
        # íŠ¹ì§• ì„ íƒ
        features = ['length_cm', 'weight_g', 'height_cm', 'growth_rate', 'variability']
        X = data_df[features].fillna(0)
        
        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = iso_forest.fit_predict(X)
        
        # ì´ìƒì¹˜ ë°ì´í„°
        anomalies = data_df[anomaly_labels == -1]
        
        print(f"ì´ìƒì¹˜ ê°ì§€: {len(anomalies)}ê°œ ({len(anomalies)/len(data_df)*100:.1f}%)")
        
        # ì‹œê°í™”
        self.visualize_anomalies(data_df, anomaly_labels)
        
        return anomalies
        
    def visualize_anomalies(self, data_df, anomaly_labels):
        """ì´ìƒì¹˜ ì‹œê°í™”"""
        plt.figure(figsize=(15, 10))
        
        # ì„œë¸Œí”Œë¡¯ 1: ì„±ì¥ë¥  vs ì‹œê°„
        plt.subplot(2, 3, 1)
        normal_data = data_df[anomaly_labels == 1]
        anomaly_data = data_df[anomaly_labels == -1]
        
        plt.scatter(normal_data.index, normal_data['growth_rate'], 
                   c='blue', alpha=0.6, label='Normal')
        plt.scatter(anomaly_data.index, anomaly_data['growth_rate'], 
                   c='red', alpha=0.8, label='Anomaly')
        plt.xlabel('Time Index')
        plt.ylabel('Growth Rate')
        plt.title('Growth Rate Anomalies')
        plt.legend()
        
        # ì„œë¸Œí”Œë¡¯ 2: í¬ê¸° ë¶„í¬
        plt.subplot(2, 3, 2)
        plt.hist(normal_data['length_cm'], bins=30, alpha=0.7, label='Normal', color='blue')
        plt.hist(anomaly_data['length_cm'], bins=30, alpha=0.7, label='Anomaly', color='red')
        plt.xlabel('Length (cm)')
        plt.ylabel('Frequency')
        plt.title('Size Distribution')
        plt.legend()
        
        # ì„œë¸Œí”Œë¡¯ 3: ë³€ë™ì„± vs ì„±ì¥ë¥ 
        plt.subplot(2, 3, 3)
        plt.scatter(normal_data['variability'], normal_data['growth_rate'], 
                   c='blue', alpha=0.6, label='Normal')
        plt.scatter(anomaly_data['variability'], anomaly_data['growth_rate'], 
                   c='red', alpha=0.8, label='Anomaly')
        plt.xlabel('Variability')
        plt.ylabel('Growth Rate')
        plt.title('Variability vs Growth Rate')
        plt.legend()
        
        # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        plt.subplot(2, 3, 4)
        correlation_matrix = data_df[['length_cm', 'weight_g', 'height_cm', 'growth_rate', 'variability']].corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('Feature Correlation')
        
        # ì‹œê³„ì—´ íŠ¸ë Œë“œ
        plt.subplot(2, 3, 5)
        if 'timestamp' in data_df.columns:
            data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
            daily_avg = data_df.groupby(data_df['timestamp'].dt.date)['growth_rate'].mean()
            plt.plot(daily_avg.index, daily_avg.values, marker='o')
            plt.title('Daily Average Growth Rate')
            plt.xticks(rotation=45)
        
        # ê±´ê°•ë„ ë¶„í¬
        plt.subplot(2, 3, 6)
        if 'health_score' in data_df.columns:
            plt.hist(normal_data['health_score'], bins=20, alpha=0.7, label='Normal', color='blue')
            plt.hist(anomaly_data['health_score'], bins=20, alpha=0.7, label='Anomaly', color='red')
            plt.xlabel('Health Score')
            plt.ylabel('Frequency')
            plt.title('Health Score Distribution')
            plt.legend()
        
        plt.tight_layout()
        plt.savefig(f'anomaly_analysis_{self.experiment_id}.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def advanced_yolo_training(self, custom_dataset_path):
        """ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ í•™ìŠµ"""
        print("ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # YOLOv8 ëª¨ë¸ ë¡œë“œ
        self.yolo_model = YOLO('yolov8m.pt')
        
        # ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹
        results = self.yolo_model.train(
            data=custom_dataset_path,  # YAML ì„¤ì • íŒŒì¼
            epochs=100,
            imgsz=640,
            batch=16,
            device=self.device,
            patience=20,
            save_period=10,
            project='aquaponics_yolo',
            name=f'experiment_{self.experiment_id}',
            
            # ê³ ê¸‰ ì„¤ì •
            optimizer='AdamW',
            lr0=0.01,
            lrf=0.1,
            momentum=0.937,
            weight_decay=0.0005,
            warmup_epochs=3,
            warmup_momentum=0.8,
            warmup_bias_lr=0.1,
            
            # ë°ì´í„° ì¦ê°•
            flipud=0.5,
            fliplr=0.5,
            mosaic=1.0,
            mixup=0.1,
            copy_paste=0.1,
            
            # ê²€ì¦ ì„¤ì •
            val=True,
            split=0.2,
            
            # ê¸°íƒ€
            verbose=True,
            seed=42
        )
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        metrics = self.yolo_model.val()
        
        print(f"YOLO í•™ìŠµ ì™„ë£Œ")
        print(f"mAP50: {metrics.box.map50:.3f}")
        print(f"mAP50-95: {metrics.box.map:.3f}")
        
        # ìµœì í™”ëœ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
        self.yolo_model.export(
            format='onnx',  # ONNX í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
            optimize=True,
            half=True,  # FP16 ìµœì í™”
            simplify=True
        )
        
        return results
        
    def generate_synthetic_data(self, num_samples=1000):
        """í•©ì„± ë°ì´í„° ìƒì„± (ë°ì´í„° ë¶€ì¡± ì‹œ)"""
        print(f"í•©ì„± ë°ì´í„° ìƒì„±: {num_samples}ê°œ ìƒ˜í”Œ")
        
        synthetic_data = []
        
        for _ in range(num_samples):
            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            base_length = np.random.normal(15, 3)
            base_weight = base_length ** 2 * np.random.normal(0.8, 0.2)
            base_height = np.random.normal(25, 5)
            
            # ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ë³€í™”
            days = np.random.randint(1, 30)
            growth_rate = np.random.normal(0.1, 0.05)
            
            # í™˜ê²½ ìš”ì¸
            temp_factor = np.random.normal(1.0, 0.1)
            ph_factor = np.random.normal(1.0, 0.1)
            light_factor = np.random.normal(1.0, 0.1)
            
            # ìµœì¢… í¬ê¸° ê³„ì‚°
            final_length = base_length * (1 + growth_rate * days * temp_factor * ph_factor)
            final_weight = base_weight * (1 + growth_rate * days * temp_factor * ph_factor) ** 2
            final_height = base_height * (1 + growth_rate * days * light_factor)
            
            # ê±´ê°•ë„ ê³„ì‚°
            health_score = np.clip(
                np.mean([temp_factor, ph_factor, light_factor]) * 
                np.random.normal(0.85, 0.1), 0, 1
            )
            
            synthetic_data.append({
                'timestamp': datetime.now() - timedelta(days=np.random.randint(0, 30)),
                'object_type': np.random.choice(['fish', 'plant']),
                'length_cm': final_length if np.random.choice(['fish', 'plant']) == 'fish' else None,
                'weight_g': final_weight if np.random.choice(['fish', 'plant']) == 'fish' else None,
                'height_cm': final_height if np.random.choice(['fish', 'plant']) == 'plant' else None,
                'growth_rate': growth_rate,
                'variability': np.random.normal(0.1, 0.05),
                'health_score': health_score,
                'confidence': np.random.uniform(0.7, 0.95),
                'environment_temp': np.random.normal(24, 2),
                'environment_ph': np.random.normal(7.0, 0.5),
                'light_intensity': np.random.normal(0.8, 0.1)
            })
            
        return pd.DataFrame(synthetic_data)
        
    def model_deployment_optimization(self):
        """ëª¨ë¸ ë°°í¬ ìµœì í™”"""
        print("ëª¨ë¸ ë°°í¬ ìµœì í™” ì‹œì‘")
        
        if self.growth_predictor is None:
            print("ì˜ˆì¸¡ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
            
        # ëª¨ë¸ ì–‘ìí™”
        self.growth_predictor.eval()
        
        # PyTorch JIT ì»´íŒŒì¼
        example_input = torch.randn(1, 6).to(self.device)
        traced_model = torch.jit.trace(self.growth_predictor, example_input)
        traced_model.save(f'traced_growth_predictor_{self.experiment_id}.pt')
        
        # ëª¨ë¸ í¬ê¸° ìµœì í™”
        original_size = sum(p.numel() for p in self.growth_predictor.parameters())
        
        # í”„ë£¨ë‹ (ê°€ì¤‘ì¹˜ ì œê±°)
        import torch.nn.utils.prune as prune
        
        for module in self.growth_predictor.modules():
            if isinstance(module, nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=0.3)
                prune.remove(module, 'weight')
                
        pruned_size = sum(p.numel() for p in self.growth_predictor.parameters() if p.requires_grad)
        
        print(f"ëª¨ë¸ í¬ê¸° ìµœì í™”: {original_size} â†’ {pruned_size} ({(1-pruned_size/original_size)*100:.1f}% ê°ì†Œ)")
        
        # ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
        torch.save(self.growth_predictor.state_dict(), f'optimized_growth_predictor_{self.experiment_id}.pth')
        
    def comprehensive_analysis_report(self, data_df, anomalies_df):
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±")
        
        report = {
            'experiment_id': self.experiment_id,
            'analysis_date': datetime.now().isoformat(),
            'data_summary': {
                'total_records': len(data_df),
                'date_range': {
                    'start': data_df['timestamp'].min() if 'timestamp' in data_df.columns else 'N/A',
                    'end': data_df['timestamp'].max() if 'timestamp' in data_df.columns else 'N/A'
                },
                'object_types': data_df['object_type'].value_counts().to_dict() if 'object_type' in data_df.columns else {}
            },
            'anomaly_analysis': {
                'total_anomalies': len(anomalies_df),
                'anomaly_rate': len(anomalies_df) / len(data_df) * 100,
                'critical_anomalies': len(anomalies_df[anomalies_df['growth_rate'] < -0.1]) if 'growth_rate' in anomalies_df.columns else 0
            },
            'growth_patterns': {
                'avg_growth_rate': data_df['growth_rate'].mean() if 'growth_rate' in data_df.columns else 0,
                'growth_std': data_df['growth_rate'].std() if 'growth_rate' in data_df.columns else 0,
                'max_growth_rate': data_df['growth_rate'].max() if 'growth_rate' in data_df.columns else 0,
                'min_growth_rate': data_df['growth_rate'].min() if 'growth_rate' in data_df.columns else 0
            },
            'health_metrics': {
                'avg_health_score': data_df['health_score'].mean() if 'health_score' in data_df.columns else 0,
                'healthy_objects_pct': (data_df['health_score'] > 0.8).mean() * 100 if 'health_score' in data_df.columns else 0,
                'critical_health_count': (data_df['health_score'] < 0.5).sum() if 'health_score' in data_df.columns else 0
            },
            'recommendations': []
        }
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        if report['anomaly_analysis']['anomaly_rate'] > 15:
            report['recommendations'].append("ì´ìƒì¹˜ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. í™˜ê²½ ì¡°ê±´ì„ ì ê²€í•˜ì„¸ìš”.")
            
        if report['growth_patterns']['avg_growth_rate'] < 0.05:
            report['recommendations'].append("í‰ê·  ì„±ì¥ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì‚¬ë£ŒëŸ‰ê³¼ í™˜ê²½ ì¡°ê±´ì„ ì¡°ì •í•˜ì„¸ìš”.")
            
        if report['health_metrics']['healthy_objects_pct'] < 70:
            report['recommendations'].append("ê±´ê°•í•œ ê°œì²´ ë¹„ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì‹œìŠ¤í…œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
        # ë¦¬í¬íŠ¸ ì €ì¥
        with open(f'analysis_report_{self.experiment_id}.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
        print("ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ")
        return report
        
    def run_full_analysis_pipeline(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 1. ë°ì´í„° ìˆ˜ì§‘ (ì„œë²„ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ë¡œì»¬ íŒŒì¼ ì‚¬ìš©)
        data_df = self.fetch_training_data()
        if data_df is None or len(data_df) < 100:
            print("ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•©ì„± ë°ì´í„° ìƒì„±")
            data_df = self.generate_synthetic_data(1000)
            
        # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        best_params = self.hyperparameter_optimization(data_df)
        
        # 3. ì„±ì¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
        self.train_growth_predictor(data_df)
        
        # 4. ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í•™ìŠµ
        self.train_rl_agent()
        
        # 5. ì´ìƒ íƒì§€ ë¶„ì„
        anomalies_df = self.anomaly_detection_analysis(data_df)
        
        # 6. ëª¨ë¸ ìµœì í™”
        self.model_deployment_optimization()
        
        # 7. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        final_report = self.comprehensive_analysis_report(data_df, anomalies_df)
        
        # 8. ë°°í¬ìš© íŒ¨í‚¤ì§€ ì¤€ë¹„ (USB ì „ì†¡ìš©)
        deploy_dir, package_info = self.prepare_deployment_package(final_report)
        
        if deploy_dir:
            print(f"\në°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„ ì™„ë£Œ!")
            print(f"ê²½ë¡œ: {os.path.abspath(deploy_dir)}")
            print(f"USBì— ë³µì‚¬í•˜ì—¬ ì„œë²„ë¡œ ì´ë™í•˜ì„¸ìš”")
            print(f"ë°°í¬ ê°€ì´ë“œ: {deploy_dir}/DEPLOYMENT_GUIDE.md")
        
        print("ğŸ‰ ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        return final_report, deploy_dir

    def fetch_training_data(self):
        """ì„œë²„ì—ì„œ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ (ì„ íƒì )"""
        try:
            print("ğŸ“¡ ì„œë²„ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œë„...")
            response = requests.get(f"{self.server_url}/api/training_data", timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                df = pd.DataFrame(data['records'])
                print(f"ì„œë²„ì—ì„œ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘: {len(df)}ê°œ ë ˆì½”ë“œ")
                return df
            else:
                print(f"ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"ğŸ”Œ ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (ì˜¤í”„ë¼ì¸ ëª¨ë“œ): {e}")
            
            # ë¡œì»¬ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ì‹œë„
            local_files = ['aquaponics_data.xlsx', 'training_data.csv', 'sample_data.json']
            for file_path in local_files:
                if os.path.exists(file_path):
                    print(f"ë¡œì»¬ íŒŒì¼ ì‚¬ìš©: {file_path}")
                    if file_path.endswith('.xlsx'):
                        return pd.read_excel(file_path)
                    elif file_path.endswith('.csv'):
                        return pd.read_csv(file_path)
                    elif file_path.endswith('.json'):
                        with open(file_path, 'r') as f:
                            data = json.load(f)
                            return pd.DataFrame(data)
            
            print("ë¡œì»¬ ë°ì´í„° íŒŒì¼ë„ ì—†ìŒ")
            return None
        except Exception as e:
            print(f"ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None
        
    def prepare_deployment_package(self, report):
        """ë°°í¬ìš© íŒ¨í‚¤ì§€ ì¤€ë¹„ (USB ì „ì†¡ìš©)"""
        print("ë°°í¬ìš© íŒ¨í‚¤ì§€ ì¤€ë¹„ ì¤‘...")
        
        # ë°°í¬ ë””ë ‰í† ë¦¬ ìƒì„±
        deploy_dir = f"deployment_package_{self.experiment_id}"
        os.makedirs(deploy_dir, exist_ok=True)
        os.makedirs(f"{deploy_dir}/models", exist_ok=True)
        os.makedirs(f"{deploy_dir}/configs", exist_ok=True)
        os.makedirs(f"{deploy_dir}/reports", exist_ok=True)
        
        package_info = {
            'experiment_id': self.experiment_id,
            'creation_date': datetime.now().isoformat(),
            'models': {},
            'configs': {},
            'reports': []
        }
        
        try:
            # 1. ìµœì í™”ëœ ëª¨ë¸ë“¤ ë³µì‚¬
            if self.growth_predictor is not None:
                # PyTorch ëª¨ë¸
                model_path = f"{deploy_dir}/models/growth_predictor.pth"
                torch.save(self.growth_predictor.state_dict(), model_path)
                
                # JIT ì»´íŒŒì¼ëœ ëª¨ë¸
                traced_path = f"{deploy_dir}/models/growth_predictor_traced.pt"
                if os.path.exists(f'traced_growth_predictor_{self.experiment_id}.pt'):
                    import shutil
                    shutil.copy(f'traced_growth_predictor_{self.experiment_id}.pt', traced_path)
                
                # ëª¨ë¸ ì •ë³´
                package_info['models']['growth_predictor'] = {
                    'file': 'growth_predictor.pth',
                    'traced_file': 'growth_predictor_traced.pt',
                    'input_dim': 6,
                    'output_dim': 1,
                    'description': 'ì„±ì¥ë¥  ì˜ˆì¸¡ ëª¨ë¸'
                }
                
            if self.rl_agent is not None:
                # ê°•í™”í•™ìŠµ ëª¨ë¸
                rl_path = f"{deploy_dir}/models/rl_agent.zip"
                self.rl_agent.save(rl_path)
                
                package_info['models']['rl_agent'] = {
                    'file': 'rl_agent.zip',
                    'algorithm': 'PPO',
                    'description': 'í™˜ê²½ ì œì–´ ìµœì í™” ì—ì´ì „íŠ¸'
                }
                
            if self.yolo_model is not None:
                # YOLO ëª¨ë¸
                yolo_path = f"{deploy_dir}/models/custom_yolo.pt"
                # ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œì—ì„œ ë³µì‚¬
                best_model_path = f"aquaponics_yolo/experiment_{self.experiment_id}/weights/best.pt"
                if os.path.exists(best_model_path):
                    import shutil
                    shutil.copy(best_model_path, yolo_path)
                    
                    package_info['models']['yolo'] = {
                        'file': 'custom_yolo.pt',
                        'version': 'YOLOv8m',
                        'description': 'ì»¤ìŠ¤í…€ ì•„ì¿ ì•„í¬ë‹‰ìŠ¤ ê°ì²´ íƒì§€ ëª¨ë¸'
                    }
            
            # 2. ì„¤ì • íŒŒì¼ë“¤
            # ëª¨ë¸ ì„¤ì •
            model_config = {
                'pixel_to_cm_ratio': 0.08,
                'fish_thresholds': {
                    'length_min': 5.0,
                    'length_max': 30.0,
                    'weight_min': 10.0,
                    'weight_max': 500.0
                },
                'plant_thresholds': {
                    'height_min': 10.0,
                    'height_max': 60.0
                },
                'change_detection': {
                    'threshold': 0.20,
                    'cooldown_seconds': 5.0
                }
            }
            
            with open(f"{deploy_dir}/configs/model_config.json", 'w') as f:
                json.dump(model_config, f, indent=2)
                
            package_info['configs']['model_config'] = 'model_config.json'
            
            # ìµœì í™” ì„¤ì •
            optimization_config = {
                'edge_device': {
                    'target_fps': 10,
                    'max_resolution': 640,
                    'frame_skip': 2,
                    'model_version': 'yolov5n'
                },
                'server': {
                    'batch_size': 5,
                    'queue_max_size': 50,
                    'model_timeout': 300,
                    'cleanup_interval': 3600
                }
            }
            
            with open(f"{deploy_dir}/configs/optimization_config.json", 'w') as f:
                json.dump(optimization_config, f, indent=2)
                
            package_info['configs']['optimization_config'] = 'optimization_config.json'
            
            # 3. ë¦¬í¬íŠ¸ ë° ë¶„ì„ ê²°ê³¼
            with open(f"{deploy_dir}/reports/analysis_report.json", 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
                
            package_info['reports'].append('analysis_report.json')
            
            # ì‹œê°í™” ê²°ê³¼ ë³µì‚¬
            if os.path.exists(f'anomaly_analysis_{self.experiment_id}.png'):
                import shutil
                shutil.copy(
                    f'anomaly_analysis_{self.experiment_id}.png',
                    f"{deploy_dir}/reports/anomaly_analysis.png"
                )
                package_info['reports'].append('anomaly_analysis.png')
            
            # 4. ë°°í¬ ê°€ì´ë“œ ìƒì„±
            deployment_guide = self._create_deployment_guide(package_info)
            with open(f"{deploy_dir}/DEPLOYMENT_GUIDE.md", 'w', encoding='utf-8') as f:
                f.write(deployment_guide)
            
            # 5. íŒ¨í‚¤ì§€ ì •ë³´ ì €ì¥
            with open(f"{deploy_dir}/package_info.json", 'w') as f:
                json.dump(package_info, f, indent=2, default=str)
            
            # 6. ì²´í¬ì„¬ ìƒì„± (ë¬´ê²°ì„± í™•ì¸ìš©)
            self._generate_checksums(deploy_dir)
            
            print(f"ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„ ì™„ë£Œ: {deploy_dir}/")
            print(f"í¬í•¨ëœ íŒŒì¼ë“¤:")
            for root, dirs, files in os.walk(deploy_dir):
                for file in files:
                    rel_path = os.path.relpath(os.path.join(root, file), deploy_dir)
                    print(f"   {rel_path}")
            
            return deploy_dir, package_info
            
        except Exception as e:
            print(f"íŒ¨í‚¤ì§€ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return None, None
    
    def _create_deployment_guide(self, package_info):
        """ë°°í¬ ê°€ì´ë“œ ë¬¸ì„œ ìƒì„±"""
        guide = f"""
# ì•„ì¿ ì•„í¬ë‹‰ìŠ¤ ëª¨ë¸ ë°°í¬ ê°€ì´ë“œ

## ì‹¤í—˜ ì •ë³´
- **ì‹¤í—˜ ID**: {package_info['experiment_id']}
- **ìƒì„± ë‚ ì§œ**: {package_info['creation_date']}

## í¬í•¨ëœ ëª¨ë¸ë“¤

"""
        
        for model_name, model_info in package_info['models'].items():
            guide += f"### {model_name}\n"
            guide += f"- **íŒŒì¼**: {model_info['file']}\n"
            guide += f"- **ì„¤ëª…**: {model_info['description']}\n"
            if 'traced_file' in model_info:
                guide += f"- **ìµœì í™” ë²„ì „**: {model_info['traced_file']}\n"
            guide += "\n"
        
        guide += """
## ë°°í¬ ìˆœì„œ

### 1. ì„œë²„ ë°°í¬ (ubuntu_server_low.py)
```bash
# 1. ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /opt/aquaponics/models

# 2. ëª¨ë¸ íŒŒì¼ ë³µì‚¬
cp models/* /opt/aquaponics/models/

# 3. ì„¤ì • íŒŒì¼ ë³µì‚¬
cp configs/* /opt/aquaponics/configs/

# 4. ì„œë²„ ì¬ì‹œì‘
sudo systemctl restart aquaponics-server
```

### 2. ë¼ì¦ˆë² ë¦¬íŒŒì´ ë°°í¬ (raspberry_pi_edge.py)
```bash
# 1. ê²½ëŸ‰í™”ëœ ëª¨ë¸ë§Œ ë³µì‚¬ (YOLOv5n)
scp models/yolo_optimized.pt pi@raspberrypi:/home/pi/aquaponics/

# 2. ì„¤ì • ì—…ë°ì´íŠ¸
scp configs/model_config.json pi@raspberrypi:/home/pi/aquaponics/

# 3. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
ssh pi@raspberrypi "sudo systemctl restart aquaponics-edge"
```

## ëª¨ë¸ ì„±ëŠ¥ ì •ë³´
"""
        
        # ì„±ëŠ¥ ì •ë³´ ì¶”ê°€ (reportì—ì„œ ì¶”ì¶œ)
        guide += """
- **ì˜ˆìƒ ì²˜ë¦¬ ì†ë„**: 
  - ë¼ì¦ˆë² ë¦¬íŒŒì´: ~10 FPS
  - ì„œë²„: ~30 FPS (ë°°ì¹˜ ì²˜ë¦¬)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**:
  - ë¼ì¦ˆë² ë¦¬íŒŒì´: ~1GB
  - ì„œë²„: ~2GB

## ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
1. íŒŒì¼ ê²½ë¡œ í™•ì¸
2. ê¶Œí•œ í™•ì¸ (`chmod 755`)
3. ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸

### ì„±ëŠ¥ ì €í•˜
1. í•´ìƒë„ ì¡°ì • (640px ì´í•˜)
2. FPS ì œí•œ (10fps ì´í•˜)
3. ë°°ì¹˜ í¬ê¸° ì¡°ì •
"""
        
        return guide
    
    def _generate_checksums(self, deploy_dir):
        """íŒŒì¼ ì²´í¬ì„¬ ìƒì„±"""
        import hashlib
        
        checksums = {}
        
        for root, dirs, files in os.walk(deploy_dir):
            for file in files:
                if file == 'checksums.json':
                    continue
                    
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, deploy_dir)
                
                with open(file_path, 'rb') as f:
                    content = f.read()
                    checksums[rel_path] = hashlib.sha256(content).hexdigest()
        
        with open(f"{deploy_dir}/checksums.json", 'w') as f:
            json.dump(checksums, f, indent=2)
        
        print(f"ì²´í¬ì„¬ ìƒì„± ì™„ë£Œ: {len(checksums)}ê°œ íŒŒì¼")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ê²°ê³¼ëŠ” USBë¡œ ì„œë²„ì— ì§ì ‘ ì „ì†¡ë©ë‹ˆë‹¤")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory // 1e9:.1f}GB")
    else:
        print("CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    research_system = ResearchLabSystem()
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        final_report, deploy_dir = research_system.run_full_analysis_pipeline()
        
        print("\nìµœì¢… ê²°ê³¼ ìš”ì•½:")
        print(f"ì‹¤í—˜ ID: {research_system.experiment_id}")
        print(f"ì²˜ë¦¬ëœ ë°ì´í„°: {final_report['data_summary']['total_records']}ê°œ")
        print(f"ì´ìƒì¹˜ ê°ì§€: {final_report['anomaly_analysis']['total_anomalies']}ê°œ")
        print(f"í‰ê·  ì„±ì¥ë¥ : {final_report['growth_patterns']['avg_growth_rate']:.4f}")
        
        if deploy_dir:
            print(f"\në°°í¬ íŒ¨í‚¤ì§€ ìœ„ì¹˜:")
            print(f"   {os.path.abspath(deploy_dir)}")
            print(f"\në‹¤ìŒ ë‹¨ê³„:")
            print(f"   1. ìœ„ í´ë”ë¥¼ USBì— ë³µì‚¬")
            print(f"   2. ìš°ë¶„íˆ¬ ì„œë²„ë¡œ ì´ë™")
            print(f"   3. DEPLOYMENT_GUIDE.md ì°¸ì¡°í•˜ì—¬ ë°°í¬")
            
            # ì••ì¶• íŒŒì¼ ìƒì„± ì˜µì…˜
            try:
                import zipfile
                zip_path = f"{deploy_dir}.zip"
                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(deploy_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, os.path.dirname(deploy_dir))
                            zipf.write(file_path, arcname)
                
                print(f"ì••ì¶• íŒŒì¼ë„ ìƒì„±ë¨: {zip_path}")
                
            except Exception as e:
                print(f"ì••ì¶• íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        
        print(f"\nëª¨ë“  ì‘ì—… ì™„ë£Œ! USBë¡œ ì„œë²„ì— ì „ì†¡í•˜ì„¸ìš”.")
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•œ ì¤‘ë‹¨")
    except Exception as e:
        print(f"\níŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()